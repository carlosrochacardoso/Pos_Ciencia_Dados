{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0tT0tbVhLC6"
   },
   "source": [
    "**NLP - Aula 5 - Extração de Informação e Reconhecimento de entidades nomeadas**\n",
    "===============================\n",
    "\n",
    "Durante a prática de hoje vamos testar alguns dos conceitos que vimos na aula e experimentar algumas funcionalidades da biblioteca Spacy.\n",
    "\n",
    "Caso o Spacy não esteja instalado no seu ambiente, entrar no console do Anaconda e fazer as instalações da biblioteca e do português:\n",
    "\n",
    "_conda install -c conda-forge spacy_\n",
    "\n",
    "_python -m spacy download pt_\n",
    "\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rr5q3aE0hKBK"
   },
   "source": [
    "### Conhecendo a biblioteca ###\n",
    "\n",
    "O objeto **nlp** contém o pipeline de processamento, incluindo regras específicas da linguagem para tokenização, entre outras.\n",
    "\n",
    "O objeto é criado utilizando a biblioteca para carregar a linguagem específica. No nosso exemplo vamos utilizar:\n",
    "``` python\n",
    "spacy.load('pt_core_news_sm')\n",
    "```\n",
    "\n",
    "Após a criação do objeto, podemos carregar nele um texto e o retorno é um documento que possui tokens.\n",
    "\n",
    "Crie o código para ler e imprimir o texto abaixo, seus tokens.\n",
    "\n",
    "**O Bill Gates veio a Belo Horizonte visitar a UFMG mês passado?**\n",
    "\n",
    "### A classe Token ###\n",
    "\n",
    "Os tokens tem uma série de atributos, que podem ser utilizados para vários tratamentos. A lista completa pode ser vista na documentação \n",
    "https://spacy.io/api/token#attributes\n",
    "\n",
    "Completando o código acima, para cada um dos tokens vamos imprimir as propriedades abaixo:\n",
    "\n",
    "* ent_type_  : Tipo da entidade nomeada\n",
    "* pos_ : Classificação 1 de POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entendendo diferença entre palavras e pontuações ###\n",
    "\n",
    "Como o spaCy entende que existe uma diferença entre uma palavra e uma pontuação, também podemos fazer filtragens. E se eu quisesse apenas as palavras da frase?\n",
    "\n",
    "Utilize as propriedades *orth_*(que retorna o texto de um token) e *is_punct*(função que retorna se o token é uma pontuação) para imprimir somente as palavras da frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematização ###\n",
    "\n",
    "A biblioteca SpaCy também retorna o *lemma_* na propriedade dos tokens.\n",
    "\n",
    "Imprima os *lemmas* dos tokens da nossa frase. E na sequência imprima os lemas das quatro flexões verbais: **encontrei, encontraram, encontrarão, encontrariam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E agora as entidades nomeadas da frase ###\n",
    "\n",
    "A classe **doc** retornada pelo tratamento nlp do texto retorna as *Entidades nomeadas* através de sua propriedade *doc.ents* \n",
    "\n",
    "Escreva o código para retornar a quantidade de entidades nomeadas do texto exemplo e a classificação de cada uma delas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifique o texto em inglês abaixo que tem mais entidades:\n",
    "\n",
    "Barack Obama is an American politician who served as the 44th President of the United States from 2009 to 2017. He is the first African American to have served as president, as well as the first born outside the contiguous United States.\n",
    "\n",
    "Vamos aproveitar e importar a biblioteca **displacy** para visualização amigável das entidades. https://spacy.io/usage/visualizers#jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pj-4L7dQhJi9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso tenha dúvida sobre alguma classe, pode usar a função **spacy.explain()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando o *displacy* mostre as entidades da frase abaixo:\n",
    "\n",
    "**José está se mudando para Califórnia. No dia 01/02/2020 ele irá partir.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando um modelo ###\n",
    "\n",
    "No exemplo acima a data não foi reconhecida. Outras frases como a seguinte também terão problema no reconhecimento:\n",
    "* No dia 12/01/2010 Maria foi aprovada no vestibular\n",
    "\n",
    "Podemos treinar um novo modelo para reconhecer o novo padrão de data. Esse mesmo método pode ser usado para criar modelo para novas entidades.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train_spacy(data,iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('pt')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "       \n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)           \n",
    "    return nlp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crie seu TRAIN_DATA\n",
    "\n",
    "#Chame a função de treinamento\n",
    "\n",
    "# Salve seu novo modelo\n",
    "\n",
    "#Teste com o texto inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com o texto \"No dia 12/01/2010 Maria foi aprovada no vestibular\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gerando dados a partir de um artigo ###\n",
    "\n",
    "Utilizando um artigo de notícia - Sugiro o **ca16** do Corpus **Brown**, leremos sentença a sentença e após vamos imprimir as entidades que mais apareceram na notícia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-f81c2ef15eef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mnamed_entities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtemp_entity_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtemp_named_entity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
     ]
    }
   ],
   "source": [
    "#Recupere o texto citado da Brown\n",
    "\n",
    "#Tokenize as sentenças na variável corpus\n",
    "\n",
    "named_entities = []\n",
    "for sentence in corpus:\n",
    "    temp_entity_name = ''\n",
    "    temp_named_entity = None\n",
    "    sentence = nlp(sentence)\n",
    "    for word in sentence:\n",
    "        term = word.text \n",
    "        tag = word.ent_type_\n",
    "        if tag:\n",
    "            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n",
    "            temp_named_entity = (temp_entity_name, tag)\n",
    "        else:\n",
    "            if temp_named_entity:\n",
    "                named_entities.append(temp_named_entity)\n",
    "                temp_entity_name = ''\n",
    "                temp_named_entity = None\n",
    "\n",
    "#Gerando o grafico das entidades\n",
    "entity_frame = pd.DataFrame(named_entities, \n",
    "                            columns=['Entity Name', 'Entity Type'])\n",
    "\n",
    "top_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n",
    "                           .size()\n",
    "                           .sort_values(ascending=False)\n",
    "                           .reset_index().rename(columns={0 : 'Frequency'}))\n",
    "top_entities.T.iloc[:,:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quiséssemos ver só o tipo de entidade agrupada. Gere o gráfico abaixo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerar o gráfico só do tipo de entidades"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP - Aula 3 - Similaridade Textual - Resposta.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
