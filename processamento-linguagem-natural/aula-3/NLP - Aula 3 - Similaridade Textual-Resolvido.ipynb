{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m0tT0tbVhLC6"
   },
   "source": [
    "**NLP - Aula 3 - Similaridade Textual**\n",
    "===============================\n",
    "\n",
    "Durante a prática de hoje vamos testar alguns dos conceitos que vimos na aula teórica sobre similaridade textual\n",
    "\n",
    "## Similaridade de Jaccard ##\n",
    "\n",
    "Vamos realizar o teste criando o algoritmo a partir da fórmula apresentada na aula J(X,Y) = |X∩Y| / |X∪Y| e um exemplo com a biblioteca __nltk.metrics.distance__\n",
    "\n",
    "Comparem as duas frases exemplo:\n",
    "\n",
    "\n",
    "*   AI is our friend and it has been friendly\n",
    "*   AI and humans have always been friendly\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 688,
     "status": "ok",
     "timestamp": 1561834615885,
     "user": {
      "displayName": "Keyla Macharet",
      "photoUrl": "https://lh5.googleusercontent.com/-_8tYhr4aAjo/AAAAAAAAAAI/AAAAAAAAEFk/XECa1Knjsbc/s64/photo.jpg",
      "userId": "10107990486249420180"
     },
     "user_tz": 180
    },
    "id": "pj-4L7dQhJi9",
    "outputId": "3eb558e2-b921-4ee5-dee0-58781a037d31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AI', 'is', 'our', 'friend', 'and', 'it', 'has', 'been', 'friendly']\n",
      "['AI', 'and', 'humans', 'have', 'always', 'been', 'friendly']\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "#Inclua aqui o seu próprio algoritmo para calcular a distância de Jaccard\n",
    "import nltk\n",
    "\n",
    "\n",
    "str1 = \"AI is our friend and it has been friendly\"\n",
    "str2 = \"AI and humans have always been friendly\"\n",
    "\n",
    "tk1 = str1.split()\n",
    "tk2 = str2.split()\n",
    "\n",
    "print(tk1)\n",
    "print(tk2)\n",
    "\n",
    "a = set(tk1)\n",
    "b = set(tk2)\n",
    "\n",
    "jaccard = len(a.intersection(b)) / len(a.union(b))\n",
    "\n",
    "print(jaccard)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 656,
     "status": "ok",
     "timestamp": 1561834774672,
     "user": {
      "displayName": "Keyla Macharet",
      "photoUrl": "https://lh5.googleusercontent.com/-_8tYhr4aAjo/AAAAAAAAAAI/AAAAAAAAEFk/XECa1Knjsbc/s64/photo.jpg",
      "userId": "10107990486249420180"
     },
     "user_tz": 180
    },
    "id": "XJi02kCHji-0",
    "outputId": "1fd8a1be-be34-428c-f69e-2832f0c35d74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33333333333333337\n"
     ]
    }
   ],
   "source": [
    "#Inclua aqui o resultado utilizando a biblioteca já criada\n",
    "\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "\n",
    "jd = jaccard_distance(a,b)\n",
    "similarity = 1 - jd\n",
    "\n",
    "print(similarity)\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UWjVqI5Fq_Ew"
   },
   "source": [
    "## Plágio de texto\n",
    "\n",
    "Através do cálculo da similaridade do Cosseno, com abordagem TFIDF + Bag of Words, iremos comparar 5 artigos de notícias e observarmos o percentual de similaridade que eles tem entre si e com base nisso analisarmos se houve cópia de uma notícia entre um site ou outro.\n",
    "\n",
    "A mesma abordagem poderia ser utilizada para várias outras aplicações de plágio, tais como a originalidade de um artigo, cópias entre exercícios de alunos, etc.\n",
    "\n",
    "As urls dos artigos que iremos analisar são:\n",
    "\n",
    "*  https://drive.google.com/uc?export=download&id=191Ae2usY5cEAPJvgxKpIve6Is1sqf5Lh\n",
    "*  https://drive.google.com/uc?export=download&id=1oIbaR3uYZajpFfOQlY2-Jfs_pik-YfxY\n",
    "*  https://drive.google.com/uc?export=download&id=1vSzS0ZwxN2KQjOZo4QgIIVsgtEbhtmXQ\n",
    "*  https://drive.google.com/uc?export=download&id=1Xou8jQIOk7GomEFtZmqcII_6vyiPo4Di\n",
    "*  https://drive.google.com/uc?export=download&id=1U1Le1LWAQbrclqL-oIt65T9PtXsqW1hN\n",
    "\n",
    "\n",
    "Primeira parte do nosso código será a importação de todas as bibliotecas que iremos utilizar:\n",
    "\n",
    "Para funções gerais de limpeza e tokenização\n",
    "*  NLTK\n",
    "*  string\n",
    "\n",
    "Para a leitura do conteúdo _raw_ das urls dos artigos\n",
    "*  requests\n",
    "\n",
    "Para calcular TF-IDF e a similaridade do cosseno\n",
    "*  from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "*  from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 677,
     "status": "ok",
     "timestamp": 1561914144784,
     "user": {
      "displayName": "Keyla Macharet",
      "photoUrl": "https://lh5.googleusercontent.com/-_8tYhr4aAjo/AAAAAAAAAAI/AAAAAAAAEFk/XECa1Knjsbc/s64/photo.jpg",
      "userId": "10107990486249420180"
     },
     "user_tz": 180
    },
    "id": "pe5gEpNG89Dn",
    "outputId": "6afe4757-7665-48eb-cd32-5b14b0e743b1"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import requests\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IYtmtUip9J95"
   },
   "source": [
    "### Recuperação e tratamento dos textos ###\n",
    "\n",
    "Como primeira parte do nosso desenvolvimento criaremos uma função com o nome **RetrieveAndProcessNews** que vai recuperar os conteúdos dos artigos e tratar os textos com os seguintes pré-processamentos:\n",
    "*  Tokenizar as palavras\n",
    "*  Remover stopwords\n",
    "*  Executar o processo de steeming (PorterStemmer)\n",
    "*  Remover pontuação\n",
    "*  Converter o texto para letras minúsculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04Rszmur3H_5"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "urls = ['https://drive.google.com/uc?export=download&id=191Ae2usY5cEAPJvgxKpIve6Is1sqf5Lh', \\\n",
    "    'https://drive.google.com/uc?export=download&id=1oIbaR3uYZajpFfOQlY2-Jfs_pik-YfxY', \\\n",
    "    'https://drive.google.com/uc?export=download&id=1vSzS0ZwxN2KQjOZo4QgIIVsgtEbhtmXQ', \\\n",
    "    'https://drive.google.com/uc?export=download&id=1Xou8jQIOk7GomEFtZmqcII_6vyiPo4Di', \\\n",
    "    'https://drive.google.com/uc?export=download&id=1U1Le1LWAQbrclqL-oIt65T9PtXsqW1hN']\n",
    "\n",
    "def RetrieveAndProcessNews(urls):\n",
    "    processed_texts = []\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for url in urls:\n",
    "        raw_text = requests.get(url).text\n",
    "        \n",
    "        text_cleaned = raw_text.translate(str.maketrans('','',string.punctuation)).lower()\n",
    "        \n",
    "        tokens = word_tokenize(text_cleaned)\n",
    "        \n",
    "        words = [stemmer.stem(word) for word in tokens if word not in stopwords]\n",
    "\n",
    "        result = ' '.join(words)\n",
    "        processed_texts.append(result)\n",
    "        \n",
    "    return processed_texts\n",
    "\n",
    "\n",
    "\n",
    "#for ct in clean_texts:\n",
    "#    print('[%s]\\n'%(ct))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-FwVk0xp9IUv"
   },
   "source": [
    "\n",
    "## TF-IDF e Similaridade do Cosseno ##\n",
    "\n",
    "Vamos criar uma função para calcular TF-IDF e em seguida a função para calcular a similaridade do cosseno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsKQxxrEPway"
   },
   "outputs": [],
   "source": [
    "def CalculateTFIDF(corpus):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    x = tfidf.fit_transform(corpus)\n",
    "    tfs_Values = x.toarray()\n",
    "    tfs_Term = tfidf.get_feature_names()\n",
    "    \n",
    "    return tfs_Values, tfs_Term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aacNLawiYdzx"
   },
   "outputs": [],
   "source": [
    "def CalculateAndPrintCosine(tfs_Values, filenames):\n",
    "    for f in filenames:\n",
    "        print(f, end='  ')\n",
    "    print()\n",
    "        \n",
    "    for i in range(len(filenames)):\n",
    "        print(filenames[i], end=' ')\n",
    "        for j in range(len(filenames)):\n",
    "            matrixValue = cosine_similarity([tfs_Values[i]],[tfs_Values[j]])\n",
    "            numValue = matrixValue[0][0]\n",
    "            print(numValue, end='  ')\n",
    "        print()\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ifRNeQOxPvYE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0606459  0.         0.03032295 ... 0.03032295 0.15305173 0.        ]\n",
      " [0.07638777 0.         0.03819388 ... 0.03819388 0.0321299  0.        ]\n",
      " [0.07958498 0.         0.03979249 ... 0.03979249 0.0334747  0.        ]\n",
      " [0.         0.0462642  0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.04385333 0.03891968]]\n",
      "\n",
      "['10', '100', '1040', '1135', '130', '136', '141', '15', '150', '156', '16', '160', '16260', '16270', '16393', '185', '1994', '1997', '200', '2003', '2004', '225', '23', '2392', '24', '30', '350', '37', '39', '400', '455', '51', '545', '58', '82', '87', 'abil', 'abl', 'ac', 'accept', 'account', 'achiev', 'ad', 'admit', 'afford', 'agre', 'ahead', 'alastair', 'alleg', 'allow', 'alreadi', 'although', 'among', 'amount', 'analyst', 'anglofrench', 'angri', 'announc', 'annual', 'anoth', 'anyth', 'appeal', 'appel', 'applic', 'approv', 'april', 'argument', 'around', 'arrang', 'arriv', 'ask', 'asset', 'assist', 'attack', 'august', 'author', 'avail', 'avoid', 'awar', 'back', 'bag', 'bank', 'bankruptci', 'barclay', 'basi', 'becom', 'benefit', 'berlusconi', 'bettino', 'bifu', 'bill', 'billion', 'bond', 'bribe', 'brighter', 'bring', 'brink', 'britain', 'british', 'brother', 'brown', 'buisi', 'busi', 'busiest', 'call', 'cant', 'cap', 'case', 'cash', 'categori', 'ceas', 'channel', 'charg', 'choos', 'chri', 'clean', 'clear', 'club', 'cochairman', 'collaps', 'come', 'commerci', 'compani', 'compet', 'competit', 'complex', 'compris', 'compromis', 'comput', 'confalonieri', 'confer', 'confirm', 'connect', 'consider', 'consolid', 'constitut', 'construct', 'convert', 'convict', 'cooper', 'core', 'corrupt', 'cost', 'could', 'court', 'coy', 'craxi', 'creditor', 'crimin', 'crippl', 'crosschannel', 'current', 'custodi', 'date', 'david', 'deal', 'debt', 'debtforequ', 'decemb', 'decid', 'decis', 'depreci', 'describ', 'despit', 'detail', 'develop', 'dickson', 'difficult', 'dilut', 'director', 'discuss', 'dividend', 'document', 'domin', 'donat', 'done', 'doomsday', 'dwindl', 'earli', 'earlier', 'east', 'econom', 'economi', 'eight', 'end', 'english', 'entitl', 'equiti', 'european', 'eurotunnel', 'even', 'exist', 'expect', 'expedit', 'explor', 'extend', 'face', 'fals', 'far', 'farreach', 'fedel', 'fee', 'ferri', 'fifti', 'fight', 'finalis', 'financ', 'financi', 'find', 'fininvest', 'firm', 'first', 'fish', 'fix', 'flow', 'follow', 'footbal', 'former', 'formula', 'forward', 'franc', 'fraud', 'free', 'french', 'friday', 'frontend', 'full', 'futur', 'gage', 'game', 'get', 'give', 'given', 'global', 'go', 'gone', 'grant', 'ground', 'group', 'guarante', 'guilt', 'half', 'halfyear', 'happen', 'happi', 'hear', 'help', 'high', 'highli', 'hold', 'holder', 'home', 'hope', 'hous', 'howev', 'illeg', 'illicit', 'im', 'immedi', 'imper', 'includ', 'incom', 'individu', 'ingeni', 'insensit', 'insolv', 'inspector', 'instruct', 'instrument', 'intent', 'interest', 'interim', 'intern', 'invest', 'investig', 'involv', 'issu', 'itali', 'italian', 'job', 'john', 'judg', 'judgement', 'june', 'justic', 'justifi', 'keep', 'larg', 'larger', 'last', 'late', 'later', 'law', 'lawyer', 'lead', 'leak', 'least', 'leav', 'legal', 'lend', 'leonard', 'less', 'level', 'lifelin', 'like', 'link', 'lloyd', 'london', 'longawait', 'longterm', 'look', 'lord', 'lost', 'low', 'made', 'main', 'major', 'make', 'mark', 'market', 'massiv', 'maximum', 'may', 'mean', 'measur', 'medium', 'mediumterm', 'meet', 'milan', 'mill', 'million', 'minist', 'monday', 'month', 'morgan', 'morn', 'morton', 'mountain', 'move', 'mr', 'multifacet', 'name', 'natur', 'nearli', 'necessari', 'need', 'negoti', 'new', 'news', 'next', 'nine', 'nobodi', 'none', 'note', 'noth', 'number', 'offenc', 'offer', 'offic', 'offshor', 'one', 'oper', 'outstand', 'owe', 'own', 'paper', 'pari', 'particip', 'patient', 'patrick', 'pay', 'payment', 'penc', 'per', 'percent', 'perform', 'place', 'plan', 'plc', 'point', 'polit', 'politician', 'ponsol', 'posit', 'potenti', 'pound', 'presid', 'press', 'presum', 'presumpt', 'pretti', 'primarili', 'prime', 'probabl', 'proceed', 'profit', 'prosecut', 'prospect', 'provid', 'publish', 'purpos', 'put', 'question', 'rais', 'rake', 'rang', 'reap', 'recognis', 'reduc', 'reject', 'rel', 'remov', 'report', 'repres', 'request', 'requir', 'respond', 'restructur', 'result', 'resum', 'return', 'revenu', 'risk', 'riskfre', 'roll', 'rout', 'royal', 'rule', 'run', 'safekeep', 'said', 'sale', 'salomon', 'say', 'scale', 'scenario', 'scotland', 'scratch', 'search', 'secret', 'secur', 'see', 'seen', 'seiz', 'senior', 'sent', 'seriou', 'seven', 'sfo', 'share', 'sharehold', 'signific', 'silvio', 'simon', 'simultan', 'six', 'small', 'sold', 'sourc', 'specul', 'speedi', 'spring', 'stabil', 'stabilis', 'staff', 'stage', 'stanley', 'state', 'statement', 'steadi', 'stem', 'step', 'still', 'stock', 'stream', 'strengthen', 'strong', 'submit', 'subscrib', 'substanti', 'succeed', 'success', 'sum', 'surreptiti', 'suspect', 'suspend', 'sustain', 'swap', 'syndic', 'system', 'take', 'taken', 'talk', 'ten', 'term', 'thought', 'three', 'throw', 'time', 'today', 'told', 'total', 'trade', 'tradit', 'transfer', 'transit', 'treat', 'trial', 'tsb', 'tuesday', 'tunnel', 'tv', 'two', 'tycoon', 'uncertain', 'uncertainti', 'underway', 'union', 'unit', 'unlik', 'unpaid', 'unspecif', 'urg', 'us', 'use', 'volum', 'want', 'warrant', 'wednesday', 'week', 'welcom', 'well', 'whether', 'wide', 'widerang', 'will', 'wipe', 'within', 'without', 'work', 'worker', 'worldwid', 'worth', 'would', 'wrangl', 'wrestl', 'year', 'yet']\n"
     ]
    }
   ],
   "source": [
    "processed_texts = RetrieveAndProcessNews(urls)\n",
    "tfs_Values, tfs_Term = CalculateTFIDF(processed_texts)\n",
    "\n",
    "print(tfs_Values)\n",
    "print()\n",
    "print(tfs_Term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iy2w9xPwS9OL"
   },
   "source": [
    "##  Hora de criar a função Main ##\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1993,
     "status": "error",
     "timestamp": 1561916375268,
     "user": {
      "displayName": "Keyla Macharet",
      "photoUrl": "https://lh5.googleusercontent.com/-_8tYhr4aAjo/AAAAAAAAAAI/AAAAAAAAEFk/XECa1Knjsbc/s64/photo.jpg",
      "userId": "10107990486249420180"
     },
     "user_tz": 180
    },
    "id": "8h-okTL0TSyp",
    "outputId": "ecc8aa3e-2471-4fa4-846a-1abe83e38bf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1  f2  f3  f4  f5  \n",
      "f1 0.9999999999999999  0.7448447086216057  0.7684204247210125  0.17461294608359224  0.11780133878807081  \n",
      "f2 0.7448447086216057  0.9999999999999991  0.976346189554524  0.12784277969773092  0.09133686524835465  \n",
      "f3 0.7684204247210125  0.976346189554524  1.0000000000000004  0.12933443080197535  0.09724710251455776  \n",
      "f4 0.17461294608359224  0.12784277969773092  0.12933443080197535  1.0000000000000007  0.07127581415325  \n",
      "f5 0.11780133878807081  0.09133686524835465  0.09724710251455776  0.07127581415325  1.0000000000000007  \n"
     ]
    }
   ],
   "source": [
    "CalculateAndPrintCosine(tfs_Values, ['f1','f2','f3','f4','f5'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP - Aula 3 - Similaridade Textual - Resposta.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
