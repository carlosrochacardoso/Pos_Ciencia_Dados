{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP - Aula 2 - Normalização e Análise de Textos\n",
    "===============================\n",
    "\n",
    "Vamos colocar em prática todos os passos que aprendemos para análise de Texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus e Classe Text\n",
    "\n",
    "Iniciar a nossa prática importanto o _corpus_ __machado__ , listando os ids e livros que fazem parte dessa biblioteca. Para isso utilize as funções __fileids()__ e __readme()__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto como cadeia de caracteres\n",
    "\n",
    "Podemos obter um texto de um Córpus como uma sequência de caracteres utilizando o método __.raw(id)__. \n",
    "\n",
    "Escreva um trecho de código que retorne a obra __Dom Casmurro__ e exibe um trecho desse texto em uma faixa que vai do caractere 250 ao 500 utilizando o fatiamento em faixa de strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textos como palavras\n",
    "\n",
    "A biblioteca NLTK também permite retornar um texto como uma lista de tokens utilizando o método __.words(id)__.\n",
    "Escreva um trecho que código para retornar a quantidade de palavras de __Dom Casmurro__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos conferir a distrição das palavras com o código abaixo:\n",
    "\n",
    "``` python\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "fd_words = FreqDist(words_casmurro)\n",
    "fd_words.plot(20)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préprocessamento do texto\n",
    "\n",
    "### Stopwords\n",
    "\n",
    "Como são consideradas palavras irrelevantes para o entendimento do sentido de um texto, é comum essas palavras serem removidas durante o pré-processamento\n",
    "\n",
    "A biblioteca contém as __stopwords__ para língua portuguesa:\n",
    "\n",
    "``` python \n",
    "nltk.corpus.stopwords.words(’portuguese’)\n",
    "\n",
    "clean_raw_casmurro = ' '.join([word for word in raw_casmurro.split() if word not in stopwords])\n",
    "\n",
    "```\n",
    "\n",
    "### Outras etapas\n",
    "\n",
    "Para termos resultados mais assertivos, limpar as __pontuações, maiúsculas/minúsculas__ e plotar novamente o resultado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização\n",
    "\n",
    "Vamos quebrar o texto com o processo de Tokenização e plotar novamente o resultado, agora após o texto ter sido pré-processado.\n",
    "\n",
    "Lembre-se de importar a biblioteca:\n",
    "\n",
    "``` python\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumarizador de textos\n",
    "\n",
    "Vamos criar o sumarizador com o modelo baseado em regras. Ele deve seguir os seguintes passos:\n",
    "\n",
    "* Leitura do conteúdo: conteúdo textual, no nosso caso estamos utilizando um livro, mas poderia ser uma fonte Web por exemplo.\n",
    "* Préprocessamento: Realizar qualquer tipo de limpeza ou formatação que o conteúdo possa necessitar.\n",
    "* Tokenizar a texto: Pegar a entrar e quebrar em palavras individuais.\n",
    "* Pontuação: Incluir um __score__ da frequência de cada palavra da entrada e pontuar as sentenças.\n",
    "* Seleção: Escolher as top N sentenças baseadas no score.\n",
    "\n",
    "Como os nossos tokens anteriores, e a nossa frequência foi toda gerada com texto limpo, vamos recriar os tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos criar o score das sentenças criadas:\n",
    "\n",
    "Partindo da lista de palavras mais importantes que geramos no passo anterior, precisamos encontrar as __sentenças mais importantes__ do livro. \n",
    "\n",
    "Utilizaremos o dicionário __defaultdict__ para criar a lista.\n",
    "\n",
    "``` python\n",
    "from collections import defaultdict\n",
    "sentencas_importantes = defaultdict(int)\n",
    "```\n",
    "\n",
    "Agora é hora de popular o dicionário com o índice da sentença e a soma da frequência de cada palavra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para gerar o resumo, precisamos selecionar as X sentenças mais importantes do texto.\n",
    "\n",
    "``` python\n",
    "\n",
    "indexes = nlargest(10, sentence_ranks, key=sentence_ranks.get)\n",
    "final_sentences = [sentence_tokens[j] for j in sorted(indexes)]\n",
    "print(' '.join(final_sentences))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer\n",
    "\n",
    "Primeiro vamos testar o Stemmer que faz parte da biblioteca NLTK para Português.\n",
    "\n",
    "``` python\n",
    "from nltk.stem import RSLPStemmer\n",
    "```\n",
    "\n",
    "_Tokenizar_ e passar o _Stemmer_ no texto e então ver se a frequência das palavras principais muda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatização\n",
    "\n",
    "Não há uma biblioteca oficial para português. Vamos testar a biblioteca __WordNetLemmatizer__ que é para inglês. \n",
    "A título de comparação utilizaremos um livro em inglês da biblioteca Gutenberg e o _stemmer_ __PorterStemmer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
